{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063fdbd4-2105-466a-9a6d-9acfc1867d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from seaborn) (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/juan/miniforge3/envs/rapids-25/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a07820-b395-40c6-bf1c-cd04f1904563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved file paths:\n",
      " {\n",
      "  \"sales\": \"./sales.csv\",\n",
      "  \"products\": \"./products_with_category_v7_clean.csv\",\n",
      "  \"cities\": \"./cities.csv\",\n",
      "  \"countries\": \"./countries.csv\",\n",
      "  \"customers\": \"./customers.csv\",\n",
      "  \"employees\": \"./employees.csv\"\n",
      "}\n",
      "[load] sales: 6,758,125 rows Ã— 9 cols\n",
      "[load] products: 452 rows Ã— 14 cols\n",
      "[load] cities: 96 rows Ã— 4 cols\n",
      "[load] countries: 206 rows Ã— 3 cols\n",
      "[load] customers: 98,759 rows Ã— 6 cols\n",
      "[load] employees: 23 rows Ã— 8 cols\n",
      "[sales] TransactionNumber unique? True\n",
      "[sales] TotalPrice = 0.0 rows: 6,758,125\n",
      "[sales] Null SalesDate rows: 67,526\n",
      "[products] VitalityDays == 0.0: 43\n",
      "[cities] Invalid Zipcodes: 3\n",
      "[countries] Invalid CountryCode format: 0\n",
      "[customers] Duplicate CustomerID rows: 0\n",
      "[employees] Duplicate EmployeeID rows: 0\n",
      "[fk] Missing key counts: {'sales->customers': 0, 'sales->products': 0, 'sales->employees': 0, 'customers->cities': 0, 'employees->cities': 0, 'cities->countries': 0}\n",
      "[audit] Data dictionary â†’ clean/report/nb01/tables/data_dictionary_nb01.csv\n",
      "[merge] Rows: 6,758,125 | Missing product info: 0 | Missing computed total: 0\n",
      "[write] Enriched sales â†’ clean/sales_enriched.parquet\n",
      "[audit] Sales daily calendar â†’ clean/report/nb01/tables/sales_daily_calendar.csv\n",
      "[plot] clean/report/nb01/plots/missing_matrix_sales.png\n",
      "[plot] clean/report/nb01/plots/missing_matrix_products.png\n",
      "[plot] clean/report/nb01/plots/outliers_box_sales_enriched.png\n",
      "[plot] clean/report/nb01/plots/outliers_box_products.png\n",
      "[write] sales â†’ clean/sales.parquet (6,758,125 rows)\n",
      "[write] products â†’ clean/products.parquet (452 rows)\n",
      "[write] cities â†’ clean/cities.parquet (96 rows)\n",
      "[write] countries â†’ clean/countries.parquet (206 rows)\n",
      "[write] customers â†’ clean/customers.parquet (98,759 rows)\n",
      "[write] employees â†’ clean/employees.parquet (23 rows)\n",
      "[write] sales_enriched â†’ clean/sales_enriched.parquet (6,758,125 rows)\n",
      "\n",
      "âœ… All cleaned tables saved to 'clean/'.\n",
      "[audit] Saved clean/report/nb01/schema.json\n",
      "[audit] Saved clean/report/nb01/summary.json\n",
      "[audit] Saved clean/report/nb01/hashes.json\n",
      "\n",
      "ðŸŽ¯ Notebook 01 (V2) finished with audit artifacts at: clean/report/nb01\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 01 â€” Global Data Cleaning (Type & Integrity Checks) (V2)\n",
    "# Purpose:\n",
    "# - Deterministic cleaning + integrity certification\n",
    "# - DO NOT break downstream contracts (official artifacts unchanged)\n",
    "# - Emit audit artifacts for traceability (schema, summary, hashes, plots)\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Block 0 â€” Setup & Config\n",
    "# -----------------------------\n",
    "import os, sys, json, math, warnings, hashlib, time\n",
    "from typing import Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype\n",
    "\n",
    "# Reproducibility & display\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "# --- Paths (EDIT if needed)\n",
    "DATA_DIR = \".\"   # raw CSV folder\n",
    "CLEAN_DIR = \"clean\"  # official cleaned outputs (contract)\n",
    "REPORT_DIR = os.path.join(CLEAN_DIR, \"report\", \"nb01\")\n",
    "\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(REPORT_DIR, \"plots\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(REPORT_DIR, \"tables\"), exist_ok=True)\n",
    "\n",
    "# Expected file names (contracts)\n",
    "FILES = {\n",
    "    \"sales\": \"sales.csv\",\n",
    "    \"products\": \"products_with_category_v7_clean.csv\",\n",
    "    \"cities\": \"cities.csv\",\n",
    "    \"countries\": \"countries.csv\",\n",
    "    \"customers\": \"customers.csv\",\n",
    "    \"employees\": \"employees.csv\",\n",
    "}\n",
    "\n",
    "file_paths = {k: os.path.join(DATA_DIR, v) for k, v in FILES.items()}\n",
    "print(\"Resolved file paths:\\n\", json.dumps(file_paths, indent=2))\n",
    "\n",
    "def assert_files_exist(paths: Dict[str, str]) -> None:\n",
    "    missing = [name for name, p in paths.items() if not os.path.isfile(p)]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Required files not found in DATA_DIR: {missing}\")\n",
    "\n",
    "def save_json(obj: dict, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, default=str)\n",
    "    print(f\"[audit] Saved {path}\")\n",
    "\n",
    "def md5_of_file(path: str) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def dataframe_schema(df: pd.DataFrame) -> List[dict]:\n",
    "    return [{\"column\": c, \"dtype\": str(df[c].dtype)} for c in df.columns]\n",
    "\n",
    "# -----------------------------\n",
    "# Block 1 â€” Load Raw CSVs\n",
    "# -----------------------------\n",
    "assert_files_exist(file_paths)\n",
    "\n",
    "dfs: Dict[str, pd.DataFrame] = {}\n",
    "for name, path in file_paths.items():\n",
    "    dfs[name] = pd.read_csv(path)\n",
    "    print(f\"[load] {name}: {dfs[name].shape[0]:,} rows Ã— {dfs[name].shape[1]} cols\")\n",
    "\n",
    "# -----------------------------\n",
    "# Block 2 â€” Basic Type Standards\n",
    "# -----------------------------\n",
    "# Dates (tz-naive)\n",
    "dt_cols = {\n",
    "    \"sales\": [\"SalesDate\"],\n",
    "    \"products\": [\"ModifyDate\"],\n",
    "    \"employees\": [\"BirthDate\", \"HireDate\"],\n",
    "}\n",
    "for tname, cols in dt_cols.items():\n",
    "    for c in cols:\n",
    "        if c in dfs[tname].columns:\n",
    "            dfs[tname][c] = pd.to_datetime(dfs[tname][c], errors=\"coerce\")\n",
    "\n",
    "# City / Country names\n",
    "if \"CityName\" in dfs[\"cities\"].columns:\n",
    "    dfs[\"cities\"][\"CityName\"] = dfs[\"cities\"][\"CityName\"].astype(str).str.strip().str.title()\n",
    "if \"CountryName\" in dfs[\"countries\"].columns:\n",
    "    dfs[\"countries\"][\"CountryName\"] = dfs[\"countries\"][\"CountryName\"].astype(str).str.strip().str.title()\n",
    "\n",
    "# Product categorical-like columns\n",
    "cat_cols_products = [\"Category\", \"Class\", \"Resistant\", \"IsAllergic\", \"ShelfLifeStatus\"]\n",
    "for col in cat_cols_products:\n",
    "    if col in dfs[\"products\"].columns:\n",
    "        dfs[\"products\"][col] = dfs[\"products\"][col].astype(str).str.strip().str.title()\n",
    "\n",
    "# Employees gender standardization\n",
    "if \"Gender\" in dfs[\"employees\"].columns:\n",
    "    dfs[\"employees\"][\"Gender\"] = (\n",
    "        dfs[\"employees\"][\"Gender\"].astype(str).str.strip().str.upper().replace({\"FEMALE\": \"F\", \"MALE\": \"M\"})\n",
    "    )\n",
    "\n",
    "# ID casting (non-failing)\n",
    "id_cols = {\"SalesID\",\"SalesPersonID\",\"CustomerID\",\"ProductID\",\"CategoryID\",\"CityID\",\"CountryID\",\"EmployeeID\"}\n",
    "for name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if col in id_cols:\n",
    "            try:\n",
    "                dfs[name][col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# Quick numeric sanity (products Price)\n",
    "if \"Price\" in dfs[\"products\"].columns:\n",
    "    negative_prices = dfs[\"products\"].loc[dfs[\"products\"][\"Price\"].fillna(0) <= 0]\n",
    "    if len(negative_prices):\n",
    "        print(\"âš ï¸ Found products with non-positive Price. Sample:\")\n",
    "        print(negative_prices[[\"ProductID\",\"ProductName\",\"Price\"]].head(5))\n",
    "\n",
    "# -----------------------------\n",
    "# Block 3 â€” SALES integrity\n",
    "# -----------------------------\n",
    "sales = dfs[\"sales\"].copy()\n",
    "\n",
    "# TransactionNumber uniqueness (if present)\n",
    "if \"TransactionNumber\" in sales.columns:\n",
    "    print(f\"[sales] TransactionNumber unique? {sales['TransactionNumber'].is_unique}\")\n",
    "\n",
    "# Basic anomaly snapshots\n",
    "zero_price_records = sales[sales.get(\"TotalPrice\", pd.Series(dtype=float)).fillna(0) == 0.0]\n",
    "null_salesdate_records = sales[sales[\"SalesDate\"].isna()]\n",
    "print(f\"[sales] TotalPrice = 0.0 rows: {len(zero_price_records):,}\")\n",
    "print(f\"[sales] Null SalesDate rows: {len(null_salesdate_records):,}\")\n",
    "\n",
    "# Flags\n",
    "sales[\"MissingSalesDate\"] = sales[\"SalesDate\"].isna()\n",
    "sales[\"Note_TotalPrice\"] = \"Needs recalculation from products\"\n",
    "dfs[\"sales\"] = sales\n",
    "\n",
    "# -----------------------------\n",
    "# Block 4 â€” PRODUCTS integrity\n",
    "# -----------------------------\n",
    "products = dfs[\"products\"].copy()\n",
    "\n",
    "if \"VitalityDays\" in products.columns:\n",
    "    vitality_zero = products[products[\"VitalityDays\"] == 0.0]\n",
    "    print(f\"[products] VitalityDays == 0.0: {len(vitality_zero):,}\")\n",
    "\n",
    "# PK & duplicates\n",
    "if products[\"ProductID\"].duplicated().any():\n",
    "    print(\"âš ï¸ Duplicate ProductID detected â€” de-duplicating by ProductID.\")\n",
    "    products = products.drop_duplicates(subset=[\"ProductID\"])\n",
    "\n",
    "products = products.drop_duplicates()\n",
    "\n",
    "# Binary fields normalization if exist\n",
    "for bcol in [\"IsFoodProduct\", \"IsAllergic_bin\"]:\n",
    "    if bcol in products.columns:\n",
    "        try:\n",
    "            if bcol == \"IsAllergic_bin\":\n",
    "                products[bcol] = pd.to_numeric(products[bcol], errors=\"coerce\").round().astype(\"Int64\")\n",
    "            else:\n",
    "                products[bcol] = pd.to_numeric(products[bcol], errors=\"coerce\").fillna(0).astype(\"Int64\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Perishable flag from VitalityDays (>0 â†’ perishable)\n",
    "if \"VitalityDays\" in products.columns:\n",
    "    products[\"Perishable\"] = (products[\"VitalityDays\"].fillna(0) > 0).astype(\"Int64\")\n",
    "\n",
    "dfs[\"products\"] = products\n",
    "\n",
    "# -----------------------------\n",
    "# Block 5 â€” CITIES integrity\n",
    "# -----------------------------\n",
    "cities = dfs[\"cities\"].copy()\n",
    "if \"CityName\" in cities.columns:\n",
    "    cities[\"CityName\"] = cities[\"CityName\"].astype(str).str.strip().str.title()\n",
    "\n",
    "# Zipcode checks\n",
    "if \"Zipcode\" in cities.columns:\n",
    "    cities[\"Zipcode_str\"] = cities[\"Zipcode\"].astype(str)\n",
    "    invalid_zip = cities[~cities[\"Zipcode_str\"].str.match(r\"^\\d{4,6}$\")]\n",
    "    print(f\"[cities] Invalid Zipcodes: {len(invalid_zip)}\")\n",
    "    cities[\"ZipcodeValid\"] = cities[\"Zipcode_str\"].str.match(r\"^\\d{4,6}$\")\n",
    "    cities = cities.drop(columns=[\"Zipcode_str\"])\n",
    "dfs[\"cities\"] = cities\n",
    "\n",
    "# -----------------------------\n",
    "# Block 6 â€” COUNTRIES integrity\n",
    "# -----------------------------\n",
    "countries = dfs[\"countries\"].copy()\n",
    "if \"CountryName\" in countries.columns:\n",
    "    countries[\"CountryName\"] = countries[\"CountryName\"].astype(str).str.strip().str.title()\n",
    "if \"CountryCode\" in countries.columns:\n",
    "    invalid_codes = countries[~countries[\"CountryCode\"].astype(str).str.match(r\"^[A-Za-z]{2,3}$\", na=True)]\n",
    "    print(f\"[countries] Invalid CountryCode format: {len(invalid_codes)}\")\n",
    "    countries[\"CountryCode\"] = countries[\"CountryCode\"].fillna(\"UNK\")\n",
    "dfs[\"countries\"] = countries\n",
    "\n",
    "# -----------------------------\n",
    "# Block 7 â€” CUSTOMERS integrity & FK\n",
    "# -----------------------------\n",
    "customers = dfs[\"customers\"].copy()\n",
    "def _clean_text_series(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.title())\n",
    "\n",
    "for c in [\"FirstName\",\"LastName\",\"Address\"]:\n",
    "    if c in customers.columns: customers[c] = _clean_text_series(customers[c])\n",
    "\n",
    "if \"MiddleInitial\" in customers.columns:\n",
    "    customers[\"MiddleInitial_missing\"] = customers[\"MiddleInitial\"].isna()\n",
    "    customers[\"MiddleInitial\"] = customers[\"MiddleInitial\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# PK\n",
    "if \"CustomerID\" in customers.columns:\n",
    "    dup = customers[\"CustomerID\"].duplicated().sum()\n",
    "    print(f\"[customers] Duplicate CustomerID rows: {dup}\")\n",
    "\n",
    "# FK City\n",
    "if \"CityID\" in customers.columns:\n",
    "    valid_city_ids = set(dfs[\"cities\"][\"CityID\"].dropna().astype(int).unique())\n",
    "    customers[\"CityID_valid\"] = customers[\"CityID\"].astype(\"Int64\").isin(valid_city_ids)\n",
    "\n",
    "# Drop exact duplicates\n",
    "customers = customers.drop_duplicates()\n",
    "dfs[\"customers\"] = customers\n",
    "\n",
    "# -----------------------------\n",
    "# Block 8 â€” EMPLOYEES integrity & FK\n",
    "# -----------------------------\n",
    "employees = dfs[\"employees\"].copy()\n",
    "for c in [\"FirstName\",\"LastName\"]:\n",
    "    if c in employees.columns: employees[c] = _clean_text_series(employees[c])\n",
    "\n",
    "if \"EmployeeID\" in employees.columns:\n",
    "    dup_emp = employees[\"EmployeeID\"].duplicated().sum()\n",
    "    print(f\"[employees] Duplicate EmployeeID rows: {dup_emp}\")\n",
    "\n",
    "if \"CityID\" in employees.columns:\n",
    "    valid_city_ids = set(dfs[\"cities\"][\"CityID\"].dropna().astype(int).unique())\n",
    "    employees[\"CityID_valid\"] = employees[\"CityID\"].astype(\"Int64\").isin(valid_city_ids)\n",
    "\n",
    "employees = employees.drop_duplicates()\n",
    "dfs[\"employees\"] = employees\n",
    "\n",
    "# -----------------------------\n",
    "# Block 9 â€” Global FK checks\n",
    "# -----------------------------\n",
    "def check_fk(parent_df, child_df, parent_key, child_key) -> set:\n",
    "    parent_keys = set(parent_df[parent_key].dropna().astype(int).unique())\n",
    "    child_keys  = set(child_df[child_key].dropna().astype(int).unique())\n",
    "    return child_keys - parent_keys\n",
    "\n",
    "fk_report = {}\n",
    "\n",
    "if {\"CustomerID\"} <= set(dfs[\"sales\"].columns):\n",
    "    miss = check_fk(dfs[\"customers\"], dfs[\"sales\"], \"CustomerID\", \"CustomerID\")\n",
    "    fk_report[\"sales->customers\"] = len(miss)\n",
    "\n",
    "if {\"ProductID\"} <= set(dfs[\"sales\"].columns):\n",
    "    miss = check_fk(dfs[\"products\"], dfs[\"sales\"], \"ProductID\", \"ProductID\")\n",
    "    fk_report[\"sales->products\"] = len(miss)\n",
    "\n",
    "if {\"SalesPersonID\"} <= set(dfs[\"sales\"].columns):\n",
    "    miss = check_fk(dfs[\"employees\"], dfs[\"sales\"], \"EmployeeID\", \"SalesPersonID\")\n",
    "    fk_report[\"sales->employees\"] = len(miss)\n",
    "\n",
    "if \"CityID\" in dfs[\"customers\"].columns:\n",
    "    miss = check_fk(dfs[\"cities\"], dfs[\"customers\"], \"CityID\", \"CityID\")\n",
    "    fk_report[\"customers->cities\"] = len(miss)\n",
    "\n",
    "if \"CityID\" in dfs[\"employees\"].columns:\n",
    "    miss = check_fk(dfs[\"cities\"], dfs[\"employees\"], \"CityID\", \"CityID\")\n",
    "    fk_report[\"employees->cities\"] = len(miss)\n",
    "\n",
    "if \"CountryID\" in dfs[\"cities\"].columns:\n",
    "    miss = check_fk(dfs[\"countries\"], dfs[\"cities\"], \"CountryID\", \"CountryID\")\n",
    "    fk_report[\"cities->countries\"] = len(miss)\n",
    "\n",
    "print(\"[fk] Missing key counts:\", fk_report)\n",
    "\n",
    "# -----------------------------\n",
    "# Block 10 â€” Data Dictionary (auto)\n",
    "# -----------------------------\n",
    "def summarize_column(s: pd.Series) -> dict:\n",
    "    total = len(s)\n",
    "    nulls = int(s.isna().sum())\n",
    "    non_null = int(total - nulls)\n",
    "    dtype = str(s.dtype)\n",
    "    try:\n",
    "        n_unique = int(s.nunique(dropna=True))\n",
    "    except Exception:\n",
    "        n_unique = None\n",
    "    min_val = max_val = None\n",
    "    try:\n",
    "        if is_numeric_dtype(s) or is_datetime64_any_dtype(s):\n",
    "            min_val, max_val = s.min(), s.max()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        samples = s.dropna().astype(str).unique()[:3].tolist()\n",
    "    except Exception:\n",
    "        samples = []\n",
    "    return {\n",
    "        \"dtype\": dtype, \"non_null\": non_null, \"nulls\": nulls,\n",
    "        \"null_pct\": round((nulls/total)*100, 4) if total else 0.0,\n",
    "        \"n_unique\": n_unique, \"min\": min_val, \"max\": max_val,\n",
    "        \"sample_values\": samples\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for tname, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        info = summarize_column(df[col])\n",
    "        rows.append({\"table\": tname, \"column\": col, **info})\n",
    "\n",
    "data_dict = pd.DataFrame(rows).sort_values([\"table\",\"column\"]).reset_index(drop=True)\n",
    "\n",
    "key_notes = {\n",
    "    (\"sales\", \"SalesID\"): \"PK\",\n",
    "    (\"sales\", \"CustomerID\"): \"FK -> customers.CustomerID\",\n",
    "    (\"sales\", \"ProductID\"): \"FK -> products.ProductID\",\n",
    "    (\"sales\", \"SalesPersonID\"): \"FK -> employees.EmployeeID\",\n",
    "    (\"customers\", \"CustomerID\"): \"PK\",\n",
    "    (\"customers\", \"CityID\"): \"FK -> cities.CityID\",\n",
    "    (\"employees\", \"EmployeeID\"): \"PK\",\n",
    "    (\"employees\", \"CityID\"): \"FK -> cities.CityID\",\n",
    "    (\"cities\", \"CityID\"): \"PK\",\n",
    "    (\"cities\", \"CountryID\"): \"FK -> countries.CountryID\",\n",
    "    (\"countries\", \"CountryID\"): \"PK\",\n",
    "    (\"products\", \"ProductID\"): \"PK\",\n",
    "    (\"products\", \"CategoryID\"): \"(dim attribute)\",\n",
    "}\n",
    "data_dict[\"key_note\"] = data_dict.apply(lambda r: key_notes.get((r[\"table\"], r[\"column\"]), \"\"), axis=1)\n",
    "\n",
    "dict_path = os.path.join(REPORT_DIR, \"tables\", \"data_dictionary_nb01.csv\")\n",
    "data_dict.to_csv(dict_path, index=False)\n",
    "print(f\"[audit] Data dictionary â†’ {dict_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Block 11 â€” Pre-merge: recompute TotalPrice (sales Ã— products)\n",
    "# -----------------------------\n",
    "sales = dfs[\"sales\"].copy()\n",
    "prod_merge_cols = [c for c in [\"ProductID\",\"Price\",\"Perishable\",\"Category\",\"Class\"] if c in dfs[\"products\"].columns]\n",
    "products = dfs[\"products\"][prod_merge_cols].copy()\n",
    "\n",
    "merged = sales.merge(\n",
    "    products, on=\"ProductID\", how=\"left\", validate=\"many_to_one\", indicator=True\n",
    ")\n",
    "merged[\"MissingProduct\"] = merged[\"_merge\"].ne(\"both\")\n",
    "merged = merged.drop(columns=\"_merge\")\n",
    "\n",
    "# Normalize Discount\n",
    "if \"Discount\" in merged.columns:\n",
    "    merged[\"Discount\"] = pd.to_numeric(merged[\"Discount\"], errors=\"coerce\").fillna(0.0).clip(0.0, 1.0)\n",
    "else:\n",
    "    merged[\"Discount\"] = 0.0\n",
    "\n",
    "# Compute totals\n",
    "if \"Price\" in merged.columns and \"Quantity\" in merged.columns:\n",
    "    merged[\"UnitNetPrice\"] = merged[\"Price\"] * (1.0 - merged[\"Discount\"])\n",
    "    merged[\"ComputedTotalPrice\"] = (pd.to_numeric(merged[\"Quantity\"], errors=\"coerce\") * merged[\"UnitNetPrice\"]).astype(float)\n",
    "else:\n",
    "    merged[\"UnitNetPrice\"] = np.nan\n",
    "    merged[\"ComputedTotalPrice\"] = np.nan\n",
    "\n",
    "merged[\"ComputedTotalPriceMissing\"] = merged[\"ComputedTotalPrice\"].isna()\n",
    "if \"TotalPrice\" in merged.columns:\n",
    "    merged = merged.rename(columns={\"TotalPrice\": \"TotalPrice_old\"})\n",
    "merged[\"TotalPrice\"] = merged[\"ComputedTotalPrice\"].round(2)\n",
    "\n",
    "print(f\"[merge] Rows: {len(merged):,} | Missing product info: {int(merged['MissingProduct'].sum()):,} | Missing computed total: {int(merged['ComputedTotalPriceMissing'].sum()):,}\")\n",
    "\n",
    "# Save enriched sales (official location unchanged from your previous code)\n",
    "enriched_path = os.path.join(CLEAN_DIR, \"sales_enriched.parquet\")\n",
    "merged.to_parquet(enriched_path, index=False)\n",
    "print(f\"[write] Enriched sales â†’ {enriched_path}\")\n",
    "\n",
    "dfs[\"sales_enriched\"] = merged\n",
    "\n",
    "# -----------------------------\n",
    "# Block 12 â€” Continuous Calendar & Missing-Day Flags (sales daily)\n",
    "# -----------------------------\n",
    "# Build a daily calendar and flag missing days at aggregate level (useful for forecasting readiness)\n",
    "if \"SalesDate\" in sales.columns:\n",
    "    min_d, max_d = sales[\"SalesDate\"].min(), sales[\"SalesDate\"].max()\n",
    "    if pd.notna(min_d) and pd.notna(max_d):\n",
    "        calendar = pd.DataFrame({\"SalesDate\": pd.date_range(min_d.normalize(), max_d.normalize(), freq=\"D\")})\n",
    "        sales_daily = (\n",
    "            sales.assign(TotalPrice=sales.get(\"TotalPrice\", pd.Series(dtype=float)))\n",
    "                 .groupby(pd.Grouper(key=\"SalesDate\", freq=\"D\"), dropna=False)[\"SalesID\"]\n",
    "                 .count()\n",
    "                 .rename(\"daily_txn_count\").reset_index()\n",
    "        )\n",
    "        sales_daily_full = calendar.merge(sales_daily, on=\"SalesDate\", how=\"left\")\n",
    "        sales_daily_full[\"is_missing_day\"] = sales_daily_full[\"daily_txn_count\"].isna()\n",
    "        # Save to audit tables\n",
    "        cal_out = os.path.join(REPORT_DIR, \"tables\", \"sales_daily_calendar.csv\")\n",
    "        sales_daily_full.to_csv(cal_out, index=False)\n",
    "        print(f\"[audit] Sales daily calendar â†’ {cal_out}\")\n",
    "    else:\n",
    "        print(\"[calendar] SalesDate min/max not available; skipping calendar build.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Block 13 â€” Audit Plots (missingness + outliers)\n",
    "# -----------------------------\n",
    "def plot_missing_matrix(df: pd.DataFrame, title: str, out_path: str, max_rows: int = 300):\n",
    "    sample = df.head(max_rows).copy()\n",
    "    miss = sample.isna().astype(int)\n",
    "    plt.figure(figsize=(min(18, 0.25*miss.shape[1] + 6), 6))\n",
    "    sns.heatmap(miss.T, cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Row index (sample)\")\n",
    "    plt.ylabel(\"Columns\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[plot] {out_path}\")\n",
    "\n",
    "def plot_box_outliers(df: pd.DataFrame, title: str, out_path: str, cols: List[str]):\n",
    "    num_cols = [c for c in cols if c in df.columns and is_numeric_dtype(df[c])]\n",
    "    if not num_cols:\n",
    "        print(f\"[plot] No numeric columns found for outlier boxplot: {title}\")\n",
    "        return\n",
    "    plt.figure(figsize=(min(18, 2*len(num_cols)+6), 6))\n",
    "    sns.boxplot(data=df[num_cols], orient=\"h\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[plot] {out_path}\")\n",
    "\n",
    "# Missing matrix for key tables\n",
    "plot_missing_matrix(dfs[\"sales\"], \"Missingness Matrix â€” sales (head 300)\", os.path.join(REPORT_DIR, \"plots\", \"missing_matrix_sales.png\"))\n",
    "plot_missing_matrix(dfs[\"products\"], \"Missingness Matrix â€” products (head 300)\", os.path.join(REPORT_DIR, \"plots\", \"missing_matrix_products.png\"))\n",
    "\n",
    "# Outliers boxplot for common numeric fields\n",
    "box_cols_sales = [c for c in [\"Quantity\",\"Discount\",\"TotalPrice\",\"ComputedTotalPrice\",\"UnitNetPrice\"] if c in merged.columns]\n",
    "plot_box_outliers(merged, \"Outliers â€” sales_enriched (key numeric)\", os.path.join(REPORT_DIR, \"plots\", \"outliers_box_sales_enriched.png\"), box_cols_sales)\n",
    "\n",
    "box_cols_products = [c for c in [\"Price\",\"VitalityDays\"] if c in products.columns]\n",
    "plot_box_outliers(products, \"Outliers â€” products (Price/VitalityDays)\", os.path.join(REPORT_DIR, \"plots\", \"outliers_box_products.png\"), box_cols_products)\n",
    "\n",
    "# -----------------------------\n",
    "# Block 14 â€” Write Official Clean Tables (CONTRACTS)\n",
    "# -----------------------------\n",
    "# Keep names as current practice; downstream notebooks rely on these.\n",
    "for name, df in dfs.items():\n",
    "    out_path = os.path.join(CLEAN_DIR, f\"{name}.parquet\")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(f\"[write] {name} â†’ {out_path} ({len(df):,} rows)\")\n",
    "\n",
    "print(\"\\nâœ… All cleaned tables saved to 'clean/'.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Block 15 â€” Schema & Summary (audit)\n",
    "# -----------------------------\n",
    "schema_payload = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"tables\": {name: dataframe_schema(df) for name, df in dfs.items()},\n",
    "}\n",
    "save_json(schema_payload, os.path.join(REPORT_DIR, \"schema.json\"))\n",
    "\n",
    "summary_payload = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(),\n",
    "    \"row_counts\": {name: int(len(df)) for name, df in dfs.items()},\n",
    "    \"missing_pct_by_table\": {\n",
    "        name: round(float(df.isna().sum().sum() / (df.shape[0]*max(df.shape[1],1)) * 100), 4) if len(df) and df.shape[1] else 0.0\n",
    "        for name, df in dfs.items()\n",
    "    },\n",
    "    \"sales_date_range\": {\n",
    "        \"min\": str(dfs[\"sales\"][\"SalesDate\"].min()) if \"SalesDate\" in dfs[\"sales\"].columns else None,\n",
    "        \"max\": str(dfs[\"sales\"][\"SalesDate\"].max()) if \"SalesDate\" in dfs[\"sales\"].columns else None,\n",
    "    },\n",
    "    \"fk_missing_counts\": fk_report,\n",
    "}\n",
    "save_json(summary_payload, os.path.join(REPORT_DIR, \"summary.json\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Block 16 â€” Hashes of Official Artifacts\n",
    "# -----------------------------\n",
    "hashes = {}\n",
    "for fname in os.listdir(CLEAN_DIR):\n",
    "    if fname.endswith(\".parquet\"):\n",
    "        fpath = os.path.join(CLEAN_DIR, fname)\n",
    "        hashes[fname] = md5_of_file(fpath)\n",
    "\n",
    "hashes_payload = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"artifacts_md5\": hashes\n",
    "}\n",
    "save_json(hashes_payload, os.path.join(REPORT_DIR, \"hashes.json\"))\n",
    "\n",
    "print(\"\\nðŸŽ¯ Notebook 01 (V2) finished with audit artifacts at:\", REPORT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
