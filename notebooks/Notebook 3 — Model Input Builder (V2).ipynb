{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0773c41-2dd1-45cb-acdc-de1175bfb3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block 1 OK — inputs snapshot & metadata initialized.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Notebook 3 — Model Input Builder (V2)\n",
    "# Block 1 — Setup, Paths, Helpers, Snapshot\n",
    "# ============================================\n",
    "\n",
    "import os, json, gc, hashlib, warnings, textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Reproducibility & display\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.4f}\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- RUN_ID & paths\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "DATA_DIR_IN   = \"clean\"                                # outputs from NB01/NB02\n",
    "DATA_DIR_OUT  = os.path.join(DATA_DIR_IN, \"model_input\")\n",
    "REPORT_DIR    = os.path.join(\"clean\", \"report\", \"nb3\")\n",
    "PLOTS_DIR     = os.path.join(REPORT_DIR, \"plots\")\n",
    "TABLES_DIR    = os.path.join(REPORT_DIR, \"tables\")\n",
    "\n",
    "for d in [DATA_DIR_OUT, REPORT_DIR, PLOTS_DIR, TABLES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ---- Policy (consistent with NB02)\n",
    "POLICY = {\n",
    "    \"salesdate_missing_in_time_series\": \"exclude\",\n",
    "    \"salesdate_missing_in_global_kpis\": \"include\"\n",
    "}\n",
    "\n",
    "# ---- Contract (official file names must not change)\n",
    "OFFICIAL = {\n",
    "    \"daily\":         os.path.join(DATA_DIR_OUT, \"model_input_sales.parquet\"),\n",
    "    \"daily_csv\":     os.path.join(DATA_DIR_OUT, \"model_input_sales.csv\"),\n",
    "    \"tx_parquet\":    os.path.join(DATA_DIR_OUT, \"model_input_sales_transactions.parquet\"),\n",
    "    \"tx_sample_csv\": os.path.join(DATA_DIR_OUT, \"model_input_sales_transactions_sample.csv\"),\n",
    "    # clustering inputs (not strict contracts, but we publish them)\n",
    "    \"cust_parquet\":  os.path.join(DATA_DIR_OUT, \"model_input_customers.parquet\"),\n",
    "    \"cust_csv\":      os.path.join(DATA_DIR_OUT, \"model_input_customers.csv\"),\n",
    "    \"full_feat_pq\":  os.path.join(DATA_DIR_OUT, \"model_features_customers_full.parquet\"),\n",
    "    \"full_feat_csv\": os.path.join(DATA_DIR_OUT, \"model_features_customers_full.csv\"),\n",
    "    \"rfm3_feat_pq\":  os.path.join(DATA_DIR_OUT, \"model_features_customers_rfm3.parquet\"),\n",
    "    \"rfm3_feat_csv\": os.path.join(DATA_DIR_OUT, \"model_features_customers_rfm3.csv\"),\n",
    "    \"feature_catalog\": os.path.join(DATA_DIR_OUT, \"feature_catalog.json\"),\n",
    "    \"metadata\":        os.path.join(DATA_DIR_OUT, \"model_input_metadata.json\"),\n",
    "    \"manifest\":        os.path.join(DATA_DIR_OUT, \"manifest.json\"),\n",
    "    \"readme\":          os.path.join(DATA_DIR_OUT, \"README_ModelInputs.md\"),\n",
    "}\n",
    "\n",
    "# ---- Required inputs\n",
    "REQ = {\n",
    "    \"sales_enriched\": os.path.join(DATA_DIR_IN, \"sales_enriched.parquet\"),\n",
    "    \"customers\":      os.path.join(DATA_DIR_IN, \"customers.parquet\"),\n",
    "    \"products\":       os.path.join(DATA_DIR_IN, \"products.parquet\"),\n",
    "    \"cities\":         os.path.join(DATA_DIR_IN, \"cities.parquet\"),\n",
    "    \"countries\":      os.path.join(DATA_DIR_IN, \"countries.parquet\"),\n",
    "    # optional reuse\n",
    "    \"customers_rfm_opt\": os.path.join(DATA_DIR_IN, \"customers_rfm.parquet\"),\n",
    "}\n",
    "\n",
    "def assert_required(req: Dict[str, str]) -> None:\n",
    "    missing = [k for k, p in req.items() if k != \"customers_rfm_opt\" and not os.path.isfile(p)]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Missing required inputs in {DATA_DIR_IN}: {missing}\")\n",
    "\n",
    "def md5_of_file(path: str) -> str:\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def atomic_write_df(df: pd.DataFrame, path: str, kind: str = \"parquet\") -> None:\n",
    "    tmp = path + f\".{RUN_ID}.tmp\"\n",
    "    if kind == \"parquet\":\n",
    "        df.to_parquet(tmp, index=False)\n",
    "    elif kind == \"csv\":\n",
    "        df.to_csv(tmp, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported kind\")\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def save_json(obj: Any, path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def dtype_string_map(s: pd.Series) -> str:\n",
    "    # Normalize dtype names for schema reporting\n",
    "    d = str(s.dtype)\n",
    "    return d\n",
    "\n",
    "# ---- Snapshot inputs\n",
    "assert_required(REQ)\n",
    "snapshot = {}\n",
    "for k, p in REQ.items():\n",
    "    e = os.path.isfile(p)\n",
    "    if e:\n",
    "        try:\n",
    "            df = pd.read_parquet(p)\n",
    "            snapshot[k] = {\"path\": p, \"exists\": True, \"rows\": int(len(df)), \"cols\": int(df.shape[1]), \"md5\": md5_of_file(p)}\n",
    "            del df\n",
    "        except Exception as ex:\n",
    "            snapshot[k] = {\"path\": p, \"exists\": True, \"error\": str(ex)}\n",
    "    else:\n",
    "        snapshot[k] = {\"path\": p, \"exists\": False}\n",
    "\n",
    "metadata = {\n",
    "    \"notebook\": \"Notebook 3 — Model Input Builder (V2)\",\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"policy\": POLICY,\n",
    "    \"inputs_snapshot\": snapshot,\n",
    "}\n",
    "\n",
    "save_json(metadata, OFFICIAL[\"metadata\"])\n",
    "print(\"✅ Block 1 OK — inputs snapshot & metadata initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f0ac14-cd54-4a9a-a6fb-d91b8686684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] versions: {'python': '3.12.7', 'platform': 'Windows-11-10.0.26100-SP0', 'numpy': '1.26.4', 'pandas': '2.2.2', 'pyarrow': '16.1.0', 'scikit_learn': '1.5.1', 'matplotlib': '3.9.2', 'seaborn': '0.13.2'}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Block 1b — Environment snapshot (versions)\n",
    "# ============================================\n",
    "import sys, platform\n",
    "\n",
    "def _safe_ver(mod):\n",
    "    try:\n",
    "        return __import__(mod).__version__\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env_versions = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"numpy\": _safe_ver(\"numpy\"),\n",
    "    \"pandas\": _safe_ver(\"pandas\"),\n",
    "    \"pyarrow\": _safe_ver(\"pyarrow\"),\n",
    "    \"scikit_learn\": _safe_ver(\"sklearn\"),\n",
    "    \"matplotlib\": _safe_ver(\"matplotlib\"),\n",
    "    \"seaborn\": _safe_ver(\"seaborn\"),\n",
    "}\n",
    "\n",
    "# Attach to metadata (idempotent)\n",
    "with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    _meta = json.load(f)\n",
    "_meta.setdefault(\"env\", {})[\"versions\"] = env_versions\n",
    "save_json(_meta, OFFICIAL[\"metadata\"])\n",
    "\n",
    "print(\"[env] versions:\", env_versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d703fe-38c2-4290-86b7-5ca990a19df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-27 15:05:59] Loading base tables…\n",
      "[2025-08-27 15:06:01] sales_enriched: 6,758,125 rows × 20\n",
      "[2025-08-27 15:06:01] customers:      98,759 rows × 8\n",
      "[2025-08-27 15:06:01] products:       452 rows × 15\n",
      "- SalesDate missing: 1.00%\n",
      "- SalesDate range:   2018-01-01 00:00:04.070000 → 2018-05-09 23:59:59.400000\n",
      "- Negative TotalPrice: 0\n",
      "- Non-positive Quantity: 0\n",
      "- Discount out of [0,1]: 0\n",
      "[2025-08-27 15:06:03] ✅ Block 2 OK — normalized & checked.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Block 2 — Load, Normalize, Health Checks\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def log(msg): \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "log(\"Loading base tables…\")\n",
    "se = pd.read_parquet(REQ[\"sales_enriched\"])\n",
    "cu = pd.read_parquet(REQ[\"customers\"])\n",
    "pr = pd.read_parquet(REQ[\"products\"])\n",
    "\n",
    "log(f\"sales_enriched: {len(se):,} rows × {se.shape[1]}\")\n",
    "log(f\"customers:      {len(cu):,} rows × {cu.shape[1]}\")\n",
    "log(f\"products:       {len(pr):,} rows × {pr.shape[1]}\")\n",
    "\n",
    "# -- Normalize key dtypes\n",
    "se[\"SalesDate\"] = pd.to_datetime(se[\"SalesDate\"], errors=\"coerce\")\n",
    "for c in [\"TotalPrice\",\"Quantity\",\"Discount\",\"Price\",\"UnitNetPrice\",\"ComputedTotalPrice\"]:\n",
    "    if c in se.columns:\n",
    "        se[c] = pd.to_numeric(se[c], errors=\"coerce\")\n",
    "\n",
    "for c in [\"SalesID\",\"CustomerID\",\"ProductID\"]:\n",
    "    if c in se.columns:\n",
    "        se[c] = pd.to_numeric(se[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# -- Health\n",
    "salesdate_null_pct = se[\"SalesDate\"].isna().mean()*100 if \"SalesDate\" in se.columns else np.nan\n",
    "min_date = se[\"SalesDate\"].min()\n",
    "max_date = se[\"SalesDate\"].max()\n",
    "neg_total = int((se[\"TotalPrice\"] < 0).sum()) if \"TotalPrice\" in se.columns else 0\n",
    "nonpos_qty = int((se[\"Quantity\"] <= 0).sum()) if \"Quantity\" in se.columns else 0\n",
    "out_disc = int(((se[\"Discount\"] < 0) | (se[\"Discount\"] > 1)).sum()) if \"Discount\" in se.columns else 0\n",
    "\n",
    "print(f\"- SalesDate missing: {salesdate_null_pct:.2f}%\")\n",
    "print(f\"- SalesDate range:   {min_date} → {max_date}\")\n",
    "print(f\"- Negative TotalPrice: {neg_total:,}\")\n",
    "print(f\"- Non-positive Quantity: {nonpos_qty:,}\")\n",
    "print(f\"- Discount out of [0,1]: {out_disc:,}\")\n",
    "\n",
    "# -- Policy view for time series\n",
    "SE_FULL = se.copy()\n",
    "SE_TIME = se.dropna(subset=[\"SalesDate\"]).copy()\n",
    "\n",
    "# -- Persist checkpoint in metadata\n",
    "with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    meta_so_far = json.load(f)\n",
    "\n",
    "meta_so_far.setdefault(\"checkpoints\", []).append({\n",
    "    \"block\": 2,\n",
    "    \"rows_total\": int(len(SE_FULL)),\n",
    "    \"rows_time_series_view\": int(len(SE_TIME)),\n",
    "    \"salesdate_missing_pct\": float(round(salesdate_null_pct, 4)),\n",
    "    \"date_min\": str(min_date) if pd.notna(min_date) else None,\n",
    "    \"date_max\": str(max_date) if pd.notna(max_date) else None,\n",
    "})\n",
    "save_json(meta_so_far, OFFICIAL[\"metadata\"])\n",
    "\n",
    "log(\"✅ Block 2 OK — normalized & checked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452e556e-735b-4078-9d17-cb55307287db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-27 15:06:03] Building DAILY aggregate…\n",
      "[2025-08-27 15:06:08] Building TRANSACTION-level table…\n",
      "Saved DAILY → clean\\model_input\\model_input_sales.parquet & clean\\model_input\\model_input_sales.csv\n",
      "Saved TX   → clean\\model_input\\model_input_sales_transactions.parquet (+ sample CSV)\n",
      "✅ Block 3 OK — contracts validated & audit written.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Block 3 — Build DAILY + TRANSACTION datasets (with guards)\n",
    "# =========================================================\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log(msg): \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "# ---------- A) DAILY aggregate\n",
    "log(\"Building DAILY aggregate…\")\n",
    "daily_rev = (SE_TIME\n",
    "             .set_index(\"SalesDate\")\n",
    "             .resample(\"D\")[\"TotalPrice\"]\n",
    "             .sum()\n",
    "             .rename(\"TotalRevenue\"))\n",
    "\n",
    "observed_days = (SE_TIME[\"SalesDate\"].dt.floor(\"D\")\n",
    "                 .value_counts()\n",
    "                 .rename_axis(\"SalesDate\")\n",
    "                 .rename(\"__n_obs__\")\n",
    "                 .sort_index())\n",
    "\n",
    "full_idx = pd.date_range(daily_rev.index.min(), daily_rev.index.max(), freq=\"D\")\n",
    "daily = pd.DataFrame(index=full_idx)\n",
    "daily[\"TotalRevenue\"] = daily_rev.reindex(full_idx, fill_value=0.0)\n",
    "daily = daily.join(observed_days, how=\"left\").fillna({\"__n_obs__\": 0})\n",
    "daily[\"observed\"] = (daily[\"__n_obs__\"] > 0).astype(\"int8\")\n",
    "daily.index.name = \"SalesDate\"\n",
    "daily = daily.reset_index().drop(columns=[\"__n_obs__\"])\n",
    "\n",
    "# ---------- B) TRANSACTION table (lean, ML-ready)\n",
    "log(\"Building TRANSACTION-level table…\")\n",
    "keep_cols = [\n",
    "    \"SalesID\",\"SalesDate\",\"CustomerID\",\"ProductID\",\"SalesPersonID\",\"TransactionNumber\",\n",
    "    \"TotalPrice\",\"Quantity\",\"Discount\",\"Price\",\"UnitNetPrice\",\"ComputedTotalPrice\",\n",
    "    \"Category\",\"Class\",\"Perishable\",\"CityID\",\"CityName\",\"CountryID\",\"CountryName\"\n",
    "]\n",
    "keep_cols = [c for c in keep_cols if c in SE_FULL.columns]\n",
    "\n",
    "tx = SE_FULL[keep_cols].copy()\n",
    "# Domain fixes\n",
    "if \"Discount\" in tx.columns:\n",
    "    tx[\"Discount\"] = pd.to_numeric(tx[\"Discount\"], errors=\"coerce\").clip(0, 1)\n",
    "tx[\"TotalPrice\"] = pd.to_numeric(tx.get(\"TotalPrice\"), errors=\"coerce\")\n",
    "\n",
    "pre_rows = len(tx)\n",
    "tx = tx.dropna(subset=[\"SalesDate\",\"TotalPrice\"])\n",
    "post_rows = len(tx)\n",
    "\n",
    "# ---------- C) CONTRACT CHECKS (abort overwrite if fail)\n",
    "def check_daily_contract(df: pd.DataFrame) -> List[str]:\n",
    "    errs = []\n",
    "    must_have = [\"SalesDate\",\"TotalRevenue\",\"observed\"]\n",
    "    missing = [c for c in must_have if c not in df.columns]\n",
    "    if missing: errs.append(f\"daily missing columns: {missing}\")\n",
    "    if \"SalesDate\" in df.columns and df[\"SalesDate\"].isna().any():\n",
    "        errs.append(\"daily has NaT SalesDate\")\n",
    "    if \"observed\" in df.columns:\n",
    "        bad_obs = ~df[\"observed\"].isin([0,1]).all()\n",
    "        if bad_obs: errs.append(\"daily 'observed' not in {0,1}\")\n",
    "    return errs\n",
    "\n",
    "def check_tx_contract(df: pd.DataFrame) -> List[str]:\n",
    "    errs = []\n",
    "    minimal = [\"SalesID\",\"SalesDate\",\"CustomerID\",\"ProductID\",\"TotalPrice\",\"Quantity\",\"Discount\"]\n",
    "    missing = [c for c in minimal if c not in df.columns]\n",
    "    if missing: errs.append(f\"transactions missing columns: {missing}\")\n",
    "    if \"SalesDate\" in df.columns and df[\"SalesDate\"].isna().any():\n",
    "        errs.append(\"transactions contain NaT SalesDate\")\n",
    "    if \"TotalPrice\" in df.columns and (df[\"TotalPrice\"] < 0).any():\n",
    "        errs.append(\"transactions have negative TotalPrice\")\n",
    "    if \"Discount\" in df.columns and ((df[\"Discount\"] < 0) | (df[\"Discount\"] > 1)).any():\n",
    "        errs.append(\"transactions have Discount outside [0,1]\")\n",
    "    return errs\n",
    "\n",
    "# ---------- C0) Revenue reconciliation (guard)\n",
    "# Make sure DAILY and TX agree on total revenue (prevents silent drift)\n",
    "rev_daily = float(daily[\"TotalRevenue\"].sum())\n",
    "rev_tx    = float(tx[\"TotalPrice\"].sum())\n",
    "\n",
    "mismatch = []\n",
    "if not np.isclose(rev_daily, rev_tx, rtol=1e-9, atol=1e-6):\n",
    "    mismatch_msg = (f\"revenue mismatch: sum(DAILY.TotalRevenue)={rev_daily:.6f} \"\n",
    "                    f\"vs sum(TX.TotalPrice)={rev_tx:.6f}\")\n",
    "    mismatch = [mismatch_msg]\n",
    "\n",
    "# ---------- C) CONTRACT CHECKS (abort overwrite if fail)\n",
    "daily_errs = mismatch + check_daily_contract(daily)\n",
    "tx_errs    = mismatch + check_tx_contract(tx)\n",
    "all_errs   = daily_errs + tx_errs\n",
    "\n",
    "if all_errs:\n",
    "    # Write drafts to audit only, do NOT overwrite official artifacts\n",
    "    draft_daily = os.path.join(REPORT_DIR, f\"draft_daily_{RUN_ID}.parquet\")\n",
    "    draft_tx    = os.path.join(REPORT_DIR, f\"draft_transactions_{RUN_ID}.parquet\")\n",
    "    daily.to_parquet(draft_daily, index=False)\n",
    "    tx.to_parquet(draft_tx, index=False)\n",
    "    save_json({\"run_id\": RUN_ID, \"errors\": all_errs}, os.path.join(REPORT_DIR, \"contract_errors.json\"))\n",
    "    raise RuntimeError(f\"[ABORT] Data contract failed. Drafts saved to report/nb3. Errors: {all_errs}\")\n",
    "\n",
    "# ---------- D) Atomic writes of OFFICIAL artifacts\n",
    "atomic_write_df(daily, OFFICIAL[\"daily\"], kind=\"parquet\")\n",
    "atomic_write_df(daily, OFFICIAL[\"daily_csv\"], kind=\"csv\")\n",
    "atomic_write_df(tx,    OFFICIAL[\"tx_parquet\"], kind=\"parquet\")\n",
    "\n",
    "# CSV sample for human inspection\n",
    "sample_cap = 200_000\n",
    "tx_sample = tx.sample(n=min(sample_cap, len(tx)), random_state=42) if len(tx) > sample_cap else tx\n",
    "atomic_write_df(tx_sample, OFFICIAL[\"tx_sample_csv\"], kind=\"csv\")\n",
    "\n",
    "# ---------- E) Row-counts table (audit)\n",
    "row_counts = pd.DataFrame([\n",
    "    {\"artifact\":\"model_input_sales.parquet\",               \"rows\": len(daily), \"cols\": daily.shape[1]},\n",
    "    {\"artifact\":\"model_input_sales_transactions.parquet\",  \"rows\": len(tx),    \"cols\": tx.shape[1]},\n",
    "    {\"artifact\":\"sales_enriched (SE_FULL)\",                \"rows\": len(SE_FULL), \"cols\": SE_FULL.shape[1]},\n",
    "    {\"artifact\":\"sales_enriched (SE_TIME non-null dates)\", \"rows\": len(SE_TIME), \"cols\": SE_TIME.shape[1]},\n",
    "])\n",
    "row_counts.to_csv(os.path.join(TABLES_DIR, \"row_counts.csv\"), index=False)\n",
    "\n",
    "# ---------- F) Schema & hashes (audit)\n",
    "schema = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"daily\": {\n",
    "        \"columns\": [{ \"name\": c, \"dtype\": dtype_string_map(daily[c]) } for c in daily.columns],\n",
    "        \"date_min\": str(daily[\"SalesDate\"].min()),\n",
    "        \"date_max\": str(daily[\"SalesDate\"].max())\n",
    "    },\n",
    "    \"transactions\": {\n",
    "        \"columns\": [{ \"name\": c, \"dtype\": dtype_string_map(tx[c]) } for c in tx.columns],\n",
    "        \"date_min\": str(tx[\"SalesDate\"].min()),\n",
    "        \"date_max\": str(tx[\"SalesDate\"].max())\n",
    "    }\n",
    "}\n",
    "save_json(schema, os.path.join(REPORT_DIR, \"schema_out.json\"))\n",
    "\n",
    "hashes = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"inputs\": {k: v[\"md5\"] for k, v in snapshot.items() if snapshot[k].get(\"md5\")},\n",
    "    \"outputs\": {\n",
    "        \"model_input_sales.parquet\":              md5_of_file(OFFICIAL[\"daily\"]),\n",
    "        \"model_input_sales.csv\":                  md5_of_file(OFFICIAL[\"daily_csv\"]),\n",
    "        \"model_input_sales_transactions.parquet\": md5_of_file(OFFICIAL[\"tx_parquet\"]),\n",
    "        \"model_input_sales_transactions_sample.csv\": md5_of_file(OFFICIAL[\"tx_sample_csv\"]),\n",
    "    }\n",
    "}\n",
    "save_json(hashes, os.path.join(REPORT_DIR, \"hashes.json\"))\n",
    "\n",
    "last_run = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"generated_at_utc\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "    \"policy\": POLICY,\n",
    "    \"daily\": {\n",
    "        \"rows\": int(len(daily)),\n",
    "        \"date_min\": str(daily[\"SalesDate\"].min()),\n",
    "        \"date_max\": str(daily[\"SalesDate\"].max()),\n",
    "        \"observed_days\": int(daily[\"observed\"].sum()),\n",
    "        \"zero_revenue_days\": int((daily[\"TotalRevenue\"] == 0).sum())\n",
    "    },\n",
    "    \"transactions\": {\n",
    "        \"rows\": int(len(tx)),\n",
    "        \"dropped_missing_target_or_date\": int(pre_rows - post_rows),\n",
    "        \"cols\": int(tx.shape[1])\n",
    "    }\n",
    "}\n",
    "save_json(last_run, os.path.join(REPORT_DIR, \"last_run.json\"))\n",
    "\n",
    "print(f\"Saved DAILY → {OFFICIAL['daily']} & {OFFICIAL['daily_csv']}\")\n",
    "print(f\"Saved TX   → {OFFICIAL['tx_parquet']} (+ sample CSV)\")\n",
    "print(\"✅ Block 3 OK — contracts validated & audit written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68707ff-34e9-4fc0-aee4-5387b0f4ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-27 15:06:14] Preparing customer-level features (RFM + extras)…\n",
      "- Reused customers_rfm.parquet: 98,759 rows\n",
      "Saved model_input_customers → clean\\model_input\\model_input_customers.parquet & clean\\model_input\\model_input_customers.csv\n",
      "✅ Block 4 OK — customer matrix ready.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Block 4 — Customers RFM + Extras + Scaled (for NB04)\n",
    "# ======================================================\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def log(msg): \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "log(\"Preparing customer-level features (RFM + extras)…\")\n",
    "\n",
    "# Optional dims\n",
    "try:\n",
    "    ci = pd.read_parquet(REQ[\"cities\"])[[\"CityID\",\"CityName\",\"CountryID\"]].drop_duplicates()\n",
    "    co = pd.read_parquet(REQ[\"countries\"])[[\"CountryID\",\"CountryName\"]].drop_duplicates()\n",
    "except Exception:\n",
    "    ci, co = None, None\n",
    "\n",
    "# Prefer NB02 RFM if schema ok\n",
    "rfm = None\n",
    "if os.path.isfile(REQ[\"customers_rfm_opt\"]):\n",
    "    cand = pd.read_parquet(REQ[\"customers_rfm_opt\"])\n",
    "    expect = {\"CustomerID\",\"last_date\",\"frequency\",\"monetary\",\"recency_days\"}\n",
    "    if expect.issubset(cand.columns):\n",
    "        rfm = cand.copy()\n",
    "        rfm[\"CustomerID\"]  = pd.to_numeric(rfm[\"CustomerID\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        rfm[\"last_date\"]   = pd.to_datetime(rfm[\"last_date\"], errors=\"coerce\")\n",
    "        for c in [\"frequency\",\"monetary\",\"recency_days\"]:\n",
    "            rfm[c] = pd.to_numeric(rfm[c], errors=\"coerce\")\n",
    "        print(f\"- Reused customers_rfm.parquet: {len(rfm):,} rows\")\n",
    "    else:\n",
    "        print(\"- customers_rfm.parquet schema unexpected; recomputing.\")\n",
    "\n",
    "if rfm is None:\n",
    "    # Build from SE_TIME\n",
    "    txn = (SE_TIME.groupby([\"SalesID\",\"CustomerID\"], as_index=False)\n",
    "           .agg(SalesDate=(\"SalesDate\",\"max\"), TotalPrice=(\"TotalPrice\",\"sum\")))\n",
    "    ref_date = SE_TIME[\"SalesDate\"].max()\n",
    "    rfm = (txn.groupby(\"CustomerID\", as_index=False)\n",
    "             .agg(last_date=(\"SalesDate\",\"max\"),\n",
    "                  frequency=(\"SalesID\",\"count\"),\n",
    "                  monetary=(\"TotalPrice\",\"sum\")))\n",
    "    rfm[\"recency_days\"] = (ref_date - rfm[\"last_date\"]).dt.days.astype(float)\n",
    "\n",
    "# Extras\n",
    "rfm[\"avg_ticket\"] = (rfm[\"monetary\"] / rfm[\"frequency\"].replace({0: np.nan})).fillna(0.0)\n",
    "\n",
    "if \"Discount\" in SE_TIME.columns:\n",
    "    disc_txn = (SE_TIME.assign(discounted=(SE_TIME[\"Discount\"] > 0).astype(int))\n",
    "                        .groupby([\"SalesID\",\"CustomerID\"], as_index=False)\n",
    "                        .agg(discounted=(\"discounted\",\"max\")))\n",
    "    pct_disc = (disc_txn.groupby(\"CustomerID\", as_index=False)\n",
    "                        .agg(pct_discounted=(\"discounted\",\"mean\")))\n",
    "    rfm = rfm.merge(pct_disc, on=\"CustomerID\", how=\"left\")\n",
    "else:\n",
    "    rfm[\"pct_discounted\"] = np.nan\n",
    "\n",
    "if \"Category\" in SE_TIME.columns:\n",
    "    ncat = (SE_TIME.groupby(\"CustomerID\")[\"Category\"]\n",
    "                  .nunique(dropna=True)\n",
    "                  .rename(\"n_categories\")\n",
    "                  .reset_index())\n",
    "    rfm = rfm.merge(ncat, on=\"CustomerID\", how=\"left\")\n",
    "else:\n",
    "    rfm[\"n_categories\"] = np.nan\n",
    "\n",
    "# top_city / top_country (mode)\n",
    "geo = cu[[\"CustomerID\",\"CityID\"]].drop_duplicates()\n",
    "if ci is not None and co is not None:\n",
    "    geo = (geo.merge(ci, on=\"CityID\", how=\"left\", validate=\"many_to_one\")\n",
    "              .merge(co, on=\"CountryID\", how=\"left\", validate=\"many_to_one\"))\n",
    "    def _mode(s):\n",
    "        s = s.dropna().astype(str)\n",
    "        return Counter(s).most_common(1)[0][0] if not s.empty else np.nan\n",
    "    top_geo = (geo.groupby(\"CustomerID\", as_index=False)\n",
    "                  .agg(top_city=(\"CityName\", _mode),\n",
    "                       top_country=(\"CountryName\", _mode)))\n",
    "    rfm = rfm.merge(top_geo, on=\"CustomerID\", how=\"left\")\n",
    "else:\n",
    "    rfm[\"top_city\"] = np.nan\n",
    "    rfm[\"top_country\"] = np.nan\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "base_feats = [\"recency_days\",\"frequency\",\"monetary\",\"avg_ticket\",\"pct_discounted\",\"n_categories\"]\n",
    "present   = [c for c in base_feats if c in rfm.columns]\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(rfm[present].fillna(0.0).values)\n",
    "for i, c in enumerate(present):\n",
    "    rfm[f\"{c}_scaled\"] = scaled[:, i]\n",
    "\n",
    "# Save\n",
    "atomic_write_df(rfm, OFFICIAL[\"cust_parquet\"], kind=\"parquet\")\n",
    "atomic_write_df(rfm, OFFICIAL[\"cust_csv\"],     kind=\"csv\")\n",
    "\n",
    "# Update metadata\n",
    "with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "meta.setdefault(\"checkpoints\", []).append({\n",
    "    \"block\": 4,\n",
    "    \"model_input_customers\": {\n",
    "        \"rows\": int(len(rfm)),\n",
    "        \"cols\": int(rfm.shape[1]),\n",
    "        \"features\": present,\n",
    "        \"scaled_features\": [f\"{c}_scaled\" for c in present]\n",
    "    },\n",
    "    \"scaler\": {\n",
    "        \"feature_order\": present,\n",
    "        \"data_min_\": [float(x) for x in scaler.data_min_],\n",
    "        \"data_max_\": [float(x) for x in scaler.data_max_],\n",
    "        \"data_range_\": [float(x) for x in scaler.data_range_],\n",
    "    }\n",
    "})\n",
    "save_json(meta, OFFICIAL[\"metadata\"])\n",
    "\n",
    "print(f\"Saved model_input_customers → {OFFICIAL['cust_parquet']} & {OFFICIAL['cust_csv']}\")\n",
    "print(\"✅ Block 4 OK — customer matrix ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128a7b37-776e-4319-9ac8-84d9539737c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Check</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>≥ 1,000 customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>≥ 5 scaled features</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 row per CustomerID</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No NaNs in scaled features</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scaled features in [0,1]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RFM3 present</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Check  Pass\n",
       "0           ≥ 1,000 customers  True\n",
       "1         ≥ 5 scaled features  True\n",
       "2        1 row per CustomerID  True\n",
       "3  No NaNs in scaled features  True\n",
       "4    Scaled features in [0,1]  True\n",
       "5                RFM3 present  True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block 5 OK — feature matrices exported.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block 5 — Go/No-Go + Export Feature Matrices (FULL & RFM3)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def log(msg): \n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
    "\n",
    "df = pd.read_parquet(OFFICIAL[\"cust_parquet\"])\n",
    "\n",
    "BASE   = [\"recency_days\",\"frequency\",\"monetary\",\"avg_ticket\",\"pct_discounted\",\"n_categories\"]\n",
    "SCALED = [f\"{c}_scaled\" for c in BASE if f\"{c}_scaled\" in df.columns]\n",
    "RFM3   = [c for c in [\"recency_days_scaled\",\"frequency_scaled\",\"monetary_scaled\"] if c in df.columns]\n",
    "id_col = \"CustomerID\"\n",
    "\n",
    "# Go/No-Go\n",
    "checks = []\n",
    "def add_check(name, cond): checks.append({\"Check\": name, \"Pass\": bool(cond)})\n",
    "\n",
    "add_check(\"≥ 1,000 customers\", len(df) >= 1000)\n",
    "add_check(\"≥ 5 scaled features\", len(SCALED) >= 5)\n",
    "add_check(\"1 row per CustomerID\", df[id_col].isna().sum()==0 and df[id_col].nunique()==len(df))\n",
    "nan_rates = df[SCALED].isna().mean() if SCALED else pd.Series([1.0])\n",
    "add_check(\"No NaNs in scaled features\", float(nan_rates.max()) == 0.0)\n",
    "mins = df[SCALED].min() if SCALED else pd.Series([1.0])\n",
    "maxs = df[SCALED].max() if SCALED else pd.Series([0.0])\n",
    "eps = 1e-9\n",
    "add_check(\"Scaled features in [0,1]\", (mins >= -eps).all() and (maxs <= 1+eps).all())\n",
    "add_check(\"RFM3 present\", len(RFM3) == 3)\n",
    "\n",
    "go_df = pd.DataFrame(checks)\n",
    "display(go_df)\n",
    "\n",
    "# ---------- Enforce Go/No-Go\n",
    "if not bool(go_df[\"Pass\"].all()):\n",
    "    # Persist failing checklist for audit\n",
    "    fail_csv = os.path.join(TABLES_DIR, f\"go_no_go_{RUN_ID}.csv\")\n",
    "    go_df.to_csv(fail_csv, index=False)\n",
    "\n",
    "    # Log into metadata with explicit FAIL status\n",
    "    with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        _meta = json.load(f)\n",
    "    _meta.setdefault(\"checkpoints\", []).append({\n",
    "        \"block\": 5,\n",
    "        \"go_no_go\": go_df.to_dict(orient=\"records\"),\n",
    "        \"status\": \"FAIL\"\n",
    "    })\n",
    "    save_json(_meta, OFFICIAL[\"metadata\"])\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"[ABORT] Go/No-Go failed; feature matrices NOT exported. See {fail_csv} and metadata.\"\n",
    "    )\n",
    "\n",
    "# If we reach here, all checks passed → proceed to export matrices\n",
    "\n",
    "\n",
    "# Matrices\n",
    "X_full = pd.concat([df[[id_col]], df[SCALED]], axis=1)\n",
    "X_rfm3 = pd.concat([df[[id_col]], df[RFM3]], axis=1)\n",
    "\n",
    "atomic_write_df(X_full, OFFICIAL[\"full_feat_pq\"], kind=\"parquet\")\n",
    "atomic_write_df(X_full, OFFICIAL[\"full_feat_csv\"], kind=\"csv\")\n",
    "atomic_write_df(X_rfm3, OFFICIAL[\"rfm3_feat_pq\"], kind=\"parquet\")\n",
    "atomic_write_df(X_rfm3, OFFICIAL[\"rfm3_feat_csv\"], kind=\"csv\")\n",
    "\n",
    "catalog = {\n",
    "    \"row_count\": int(len(df)),\n",
    "    \"id_column\": id_col,\n",
    "    \"features_full\": SCALED,\n",
    "    \"features_rfm3\": RFM3,\n",
    "    \"nan_rates_scaled\": {k: float(v) for k, v in nan_rates.to_dict().items()} if not nan_rates.empty else {},\n",
    "    \"bounds_scaled\": {c: {\"min\": float(mins[c]), \"max\": float(maxs[c])} for c in SCALED} if len(SCALED)>0 else {},\n",
    "    \"notes\": [\n",
    "        \"Scaled with MinMax on current population.\",\n",
    "        \"RFM3 = recency_days_scaled, frequency_scaled, monetary_scaled.\"\n",
    "    ]\n",
    "}\n",
    "save_json(catalog, OFFICIAL[\"feature_catalog\"])\n",
    "\n",
    "# Update metadata\n",
    "with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "meta.setdefault(\"checkpoints\", []).append({\n",
    "    \"block\": 5,\n",
    "    \"go_no_go\": go_df.to_dict(orient=\"records\"),\n",
    "    \"features\": {\"full\": SCALED, \"rfm3\": RFM3, \"rows\": int(len(df))}\n",
    "})\n",
    "save_json(meta, OFFICIAL[\"metadata\"])\n",
    "\n",
    "print(\"✅ Block 5 OK — feature matrices exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cfc15c-67ad-40d0-81bb-27c2772448d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Manifest saved → clean\\model_input\\manifest.json\n",
      "[OK] README written → clean\\model_input\\README_ModelInputs.md\n",
      "\n",
      "=== SUMMARY ===\n",
      "Artifacts found: 15 | Missing: 0\n",
      "- clean\\model_input\\model_input_sales.parquet: rows=129, cols=3, size=4694 bytes\n",
      "- clean\\model_input\\model_input_sales_transactions.parquet: rows=6690599, cols=15, size=318517937 bytes\n",
      "- clean\\model_input\\model_input_customers.parquet: rows=98759, cols=16, size=6090888 bytes\n",
      "- clean\\model_input\\model_features_customers_full.parquet: rows=98759, cols=7, size=2935971 bytes\n",
      "- clean\\model_input\\model_features_customers_rfm3.parquet: rows=98759, cols=4, size=1753534 bytes\n",
      "\n",
      "✅ Notebook 3 (V2) complete — contracts validated & audit ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Block 6 — Manifest + README + Final Checkpoint\n",
    "# ============================================\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def summarize_parquet(path: Path):\n",
    "    info = {\"size_bytes\": path.stat().st_size}\n",
    "    try:\n",
    "        m = pq.ParquetFile(path).metadata\n",
    "        info[\"rows\"] = m.num_rows\n",
    "        info[\"cols\"] = m.num_columns\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_parquet(path)\n",
    "            info[\"rows\"], info[\"cols\"] = int(df.shape[0]), int(df.shape[1])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return info\n",
    "\n",
    "def artifact_entry(p: Path):\n",
    "    ftype = p.suffix.lstrip(\".\").lower()\n",
    "    entry = {\"file\": str(p), \"name\": p.name, \"type\": ftype, \"exists\": p.exists()}\n",
    "    if entry[\"exists\"] and ftype == \"parquet\":\n",
    "        entry.update(summarize_parquet(p))\n",
    "    if entry[\"exists\"] and ftype in {\"csv\",\"json\",\"md\"}:\n",
    "        entry[\"size_bytes\"] = p.stat().st_size\n",
    "    return entry\n",
    "\n",
    "# Manifest\n",
    "expected = [Path(v) for v in OFFICIAL.values()]\n",
    "seen = set(p.name for p in expected)\n",
    "for p in Path(DATA_DIR_OUT).iterdir():\n",
    "    if p.suffix.lower() in {\".parquet\",\".csv\",\".json\",\".md\"} and p.name not in seen:\n",
    "        expected.append(p); seen.add(p.name)\n",
    "\n",
    "manifest = {\"run_id\": RUN_ID, \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"), \"artifacts\": []}\n",
    "for p in expected:\n",
    "    manifest[\"artifacts\"].append(artifact_entry(Path(p)))\n",
    "\n",
    "save_json(manifest, OFFICIAL[\"manifest\"])\n",
    "print(f\"[OK] Manifest saved → {OFFICIAL['manifest']}\")\n",
    "\n",
    "# Final checkpoint\n",
    "with open(OFFICIAL[\"metadata\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "final_ckpt = {\n",
    "    \"block\": 6,\n",
    "    \"status\": \"complete\",\n",
    "    \"artifacts_count\": len(manifest[\"artifacts\"]),\n",
    "    \"ready_for\": {\n",
    "        \"forecasting_random_forest\": \"model_input_sales_transactions.parquet\",\n",
    "        \"forecasting_timeseries_baseline\": \"model_input_sales.parquet\",\n",
    "        \"clustering_full\": \"model_features_customers_full.parquet\",\n",
    "        \"clustering_rfm3\": \"model_features_customers_rfm3.parquet\",\n",
    "    }\n",
    "}\n",
    "meta.setdefault(\"checkpoints\", []).append(final_ckpt)\n",
    "save_json(meta, OFFICIAL[\"metadata\"])\n",
    "\n",
    "# README\n",
    "readme_text = textwrap.dedent(f\"\"\"\n",
    "# Model Inputs — Notebook 3 (V2)\n",
    "Run: {RUN_ID}\n",
    "\n",
    "## Official artifacts\n",
    "- **model_input_sales.parquet/csv** — Daily `SalesDate`, `TotalRevenue`, `observed` (0/1).\n",
    "- **model_input_sales_transactions.parquet** (+ sample CSV) — Transaction-level ML dataset.\n",
    "- **model_input_customers.parquet/csv** — RFM + extras + scaled columns.\n",
    "- **model_features_customers_full / _rfm3** — Tidy matrices for K-Means.\n",
    "- **feature_catalog.json** — Feature list, NaNs, bounds.\n",
    "- **model_input_metadata.json** — All checkpoints & snapshots.\n",
    "- **manifest.json** — Presence, sizes, row/col counts.\n",
    "\n",
    "## Data-contract guards\n",
    "- DAILY must contain: `SalesDate` (no NaT), `TotalRevenue`, `observed` ∈ {{0,1}}.\n",
    "- TRANSACTIONS must contain: `SalesID`, `SalesDate`, `CustomerID`, `ProductID`, `TotalPrice`, `Quantity`, `Discount`.\n",
    "- If any guard fails, official files are not overwritten — drafts under `clean/report/nb3/`.\n",
    "\n",
    "## Policy\n",
    "- Exclude null `SalesDate` rows from time-based metrics; seed=42; run_id={RUN_ID}.\n",
    "\"\"\").strip()\n",
    "with open(OFFICIAL[\"readme\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_text + \"\\n\")\n",
    "print(f\"[OK] README written → {OFFICIAL['readme']}\")\n",
    "\n",
    "# Console summary\n",
    "missing = [a[\"file\"] for a in manifest[\"artifacts\"] if not a[\"exists\"]]\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Artifacts found: {len(manifest['artifacts'])} | Missing: {len(missing)}\")\n",
    "for a in manifest[\"artifacts\"]:\n",
    "    if a[\"exists\"] and a[\"type\"] == \"parquet\":\n",
    "        print(f\"- {a['file']}: rows={a.get('rows')}, cols={a.get('cols')}, size={a['size_bytes']} bytes\")\n",
    "\n",
    "print(\"\\n✅ Notebook 3 (V2) complete — contracts validated & audit ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e249e39b-8f52-4e27-830c-e1a0cb6df33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[audit] wrote clean\\model_input\\hashes.json\n",
      "[audit] wrote clean\\report\\nb3\\tables\\row_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# --- NB03 add-on: hashes.json + row_counts.csv (audit hardening) ---\n",
    "import os, json, hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path(\"clean/model_input\")\n",
    "REPORT_DIR = Path(\"clean/report/nb3/tables\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = [\n",
    "    \"model_input_sales.parquet\",\n",
    "    \"model_input_sales_transactions.parquet\",\n",
    "    \"model_input_customers.parquet\",\n",
    "    \"model_features_customers_full.parquet\",\n",
    "    \"model_features_customers_rfm3.parquet\",\n",
    "]\n",
    "\n",
    "def md5sum(path, blocksize=1<<20):\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(blocksize), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# 1) hashes.json\n",
    "hashes = {}\n",
    "for fname in files:\n",
    "    p = BASE / fname\n",
    "    if p.exists():\n",
    "        hashes[fname] = md5sum(p)\n",
    "with open(BASE / \"hashes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hashes, f, indent=2)\n",
    "print(f\"[audit] wrote {BASE/'hashes.json'}\")\n",
    "\n",
    "# 2) row_counts.csv\n",
    "rows = []\n",
    "for fname in files:\n",
    "    p = BASE / fname\n",
    "    if p.exists() and p.suffix == \".parquet\":\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            m = pq.ParquetFile(p).metadata\n",
    "            rows.append({\"file\": str(p), \"rows\": m.num_rows, \"cols\": m.num_columns})\n",
    "        except Exception:\n",
    "            df = pd.read_parquet(p)\n",
    "            rows.append({\"file\": str(p), \"rows\": len(df), \"cols\": df.shape[1]})\n",
    "rc = pd.DataFrame(rows)\n",
    "rc.to_csv(REPORT_DIR / \"row_counts.csv\", index=False)\n",
    "print(f\"[audit] wrote {REPORT_DIR/'row_counts.csv'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
